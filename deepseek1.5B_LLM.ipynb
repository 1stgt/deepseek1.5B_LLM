{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4340b408-b55c-44df-9905-e803444cc269",
   "metadata": {},
   "source": [
    "# Step 1: Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff6d620-40f5-4429-9486-d12899069c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 320.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 432.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/9.7 MB 12.9 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.0/9.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.5/9.7 MB 10.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.9/9.7 MB 10.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.2/9.7 MB 9.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.7/9.7 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.4/9.7 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.7 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.5/9.7 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.7 MB 11.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.8/9.7 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.4/9.7 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.8/9.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.5/9.7 MB 12.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "   ---------------------------------------- 0.0/450.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 450.7/450.7 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 303.8/303.8 kB 19.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 13.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.27.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch tensorflow flax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db225f-136e-452a-bec5-8fdd4aecaa87",
   "metadata": {},
   "source": [
    "# Step 2: Importing  the library for the \"DeepSeek-R1-Distill-Qwen-1.5B\" model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440065df-54e5-4946-9457-af3c81054404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee79bf08-83d4-4ca1-9a24-d6d92f1aa738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f0bfb-bd5e-4cf7-af5b-f58f069795ce",
   "metadata": {},
   "source": [
    "## Download the model and tokenizer\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c2e095-27e3-4ac4-a7d3-a91538f4db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b219a8-3f9c-4edd-a178-8a45b35442db",
   "metadata": {},
   "source": [
    "# saving the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6440c5a4-a31e-47ef-99ec-42641e515fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save the model in the current directory\n",
    "local_model_directory = \".\" \n",
    "\n",
    "# Save the model and tokenizer to the local directory\n",
    "model.save_pretrained(local_model_directory)\n",
    "tokenizer.save_pretrained(local_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f58bce-4490-4981-829b-87fe9ed56d23",
   "metadata": {},
   "source": [
    "# Loading the Model and Tokenizer from our system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e57528-e2e5-44ed-a5ea-884dd3fa24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c95427a-b3bd-4158-9ebb-25045061e77b",
   "metadata": {},
   "source": [
    "# Now let’s try to generate some response from the model that we have saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3160f041-0402-4187-9326-37041bcec743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(input_text):\n",
    "    # Encode the input text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    # Create an attention mask \n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    \n",
    "    # Apply generation with more refined parameters\n",
    "    outputs = model.generate(inputs,\n",
    "                             attention_mask=attention_mask,\n",
    "                             max_length=250,            # Allow for more room in the response\n",
    "                             num_return_sequences=1,    # Generate 1 response\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             temperature=0.5,           # Set the temperature lower for more controlled output\n",
    "                             top_p=0.7,                 # Use nucleus sampling for diversity\n",
    "                             top_k=50,                  # Restrict sampling to top 50 most likely next tokens\n",
    "                             no_repeat_ngram_size=3)    # Prevent repeating n-grams\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5eb14ca-a337-4173-8868-2cbfbd695cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with the model (type 'exit' to stop):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  If there are 5 apples and you take away 3, how many do you have?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: If there are 5 apples and you take away 3, how many do you have? (This is a riddle.)\n",
      "\n",
      "Okay, so I have this riddle here: If there are five apples and I take away three, how do I know how many I have? Hmm, let me try to figure this out. \n",
      "\n",
      "First, I know that when you take something away, you're reducing the total. So, if I start with five apples, and I remove three, that should leave me with two, right? But wait, the riddle is asking how I know the answer. Maybe it's not as straightforward as just subtracting three from five. \n",
      "\n",
      "Let me think about the wording again: \"If there were five apples...\" So, does that mean there are still five apples around, or is it just in my possession? Hmm. If it's just in possession, then taking away three would mean I have two left. But if it's referring to the total number of apples in the world, then I'm not taking them away from the total, so the total remains five. But that seems a bit more complicated. \n",
      "\n",
      "Wait, the question is phrased as a riddles, so maybe it's\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  If a human brain is replaced with AI, how will it behave? Share your thoughts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: If a human brain is replaced with AI, how will it behave? Share your thoughts. (Please answer in English)\n",
      "</think>\n",
      "\n",
      "If a person were to replace their human brain with AI (Artificial Intelligence), the AI would exhibit behaviors that are a combination of human and AI characteristics. Here are some potential outcomes:\n",
      "\n",
      "1. **Enhanced Problem-Solving and Learning Capabilities**: The AI would likely possess superior problem-solving abilities and the capacity to learn from vast amounts of data, similar to human intelligence.\n",
      "\n",
      "2. **Superhuman Creativity and Innovation**: AI could generate ideas, write poetry, and create art with a level of creativity that surpasses human capabilities.\n",
      "\n",
      "3. **Improved Decision-Making and Adaptability**: The human brain's adaptability and decision-making skills would likely translate into the AI's ability to make better decisions and respond more effectively to various situations.\n",
      "\n",
      "4. **Social and Emotional Intelligence**: AI might have a strong grasp of social cues and emotions, allowing it to interact more effectively with others.\n",
      "\n",
      "5. **Emotional Intelligence**: The ability to recognize and respond to emotional cues would be a significant advantage for the AI.\n",
      "\n",
      "6. **Ethical and Moral Concerns**: While the AI could perform tasks that humans cannot, it\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  How can you say AI would have better understanding of emotions. It does not have a heart?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: How can you say AI would have better understanding of emotions. It does not have a heart? Or is it just a matter of the way the AI is trained? I'm a bit confused.\n",
      "</think>\n",
      "\n",
      "AI's ability to understand emotions is a result of its training data and algorithms. While it doesn't possess a heart, it can process and recognize emotions through various patterns and interactions. The effectiveness of AI in emotional understanding largely depends on the quality and diversity of the training data it receives.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  But emotions vary from situations and persons, there is no such pattern of the intensity. I don't think that can be trained in any way. What would you say about this?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: But emotions vary from situations and persons, there is no such pattern of the intensity. I don't think that can be trained in any way. What would you say about this? Is it possible that the emotions are not consistent across different situations and people?\n",
      "\n",
      "I think that the way we process emotions is different for each person, but maybe that's not the case. I'm not sure. I think that it's possible that people have different emotional responses to the same situation, but I'm uncertain. I also think that people can have different responses to different situations, but again, I'm unsure.\n",
      "\n",
      "I think the way emotions are processed is different across people, but perhaps that's just a generalization. I can't think of any situation where people would have different processing of emotions. Maybe that's a good point. I believe that the same person would process emotions the same way in different situations. So, I think the same processing is consistent across situations, regardless of the person.\n",
      "\n",
      "But I'm still not sure about the consistency across people. I wonder if there are studies or data that support this. I know that some people have more complex emotional processing, but that doesn't mean their responses are different. I guess it's about the same\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Chat loop\n",
    "print(\"Chat with the model (type 'exit' to stop):\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_model(user_input)\n",
    "    print(f\"Model: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baecb26b-0003-4f59-802b-e8549ee74ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with the model (type 'exit' to stop):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Who is the current president of the United States?¶\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Who is the current president of the United States?¶\n",
      "</think>\n",
      "\n",
      "As of my last update in July 2024, the current President of the People's Republic of China is Mr. Xi Jinping. For the most current information,\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Who is the current president of the india?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Who is the current president of the india? (2023)\n",
      "</think>\n",
      "\n",
      "As of my last update in July 2000, the current President of India is Rajya Sabha, the 105th President elected in\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Who wrote \"Pride and Prejudice\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Who wrote \"Pride and Prejudice\"? Who wrote \"The Adventures of Huckleberry Finn\"? Who did you write? Who wrote the novel \"The Great Gatsby\"? Who was the author of \"The Catcher in the Rye\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  If there are 5 apples and you take away 3, how many do you have?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: If there are 5 apples and you take away 3, how many do you have? If you have 5, take away... wait, the question is a bit confusing. Let me try to figure it out. So, the first\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the square root of 144?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: What is the square root of 144? Can you explain the square roots of 25 and 36?\n",
      "\n",
      "To what degree of precision should I round my answer? Let me see.\n",
      "\n",
      "Alright, so I have this math problem\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hey you are a teacher and i will ask you a question and you need to answer correctly do nt send wrong answer \"\"What is the square root of 144?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: hey you are a teacher and i will ask you a question and you need to answer correctly do nt send wrong answer \"\"What is the square root of 144?\" Please answer and explain.\n",
      "</think>\n",
      "\n",
      "The square root is the number that\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Chat loop\n",
    "print(\"Chat with the model (type 'exit' to stop):\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_model(user_input)\n",
    "    print(f\"Model: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff19194-10fa-4e1a-b54c-3451e38eb40b",
   "metadata": {},
   "source": [
    "## 1. General Knowledge\n",
    "### Who is the current president of the United States?\n",
    "### What is the capital of Japan?\n",
    "### Who wrote \"Pride and Prejudice\"?\n",
    "## 2. Logical Reasoning\n",
    "### If there are 5 apples and you take away 3, how many do you have?\n",
    "### What is the next number in this sequence: 2, 4, 8, 16, 32, ___?\n",
    "### A is taller than B. B is taller than C. Who is the tallest?\n",
    "## 3. Math\n",
    "### What is the square root of 144?\n",
    "### Solve this equation: 3x + 5 = 20.\n",
    "### What is the area of a circle with a radius of 7 units?\n",
    "## 4. Conversational/Contextual Understanding\n",
    "### What do you think about the future of AI?\n",
    "### Can you explain the difference between machine learning and deep learning?\n",
    "### How would you recommend learning Python for data science?\n",
    "## 5. Code/Programming\n",
    "### Can you write a Python function to reverse a list?\n",
    "### What is the difference between a list and a tuple in Python?\n",
    "### Write a SQL query to select all records from a table where the column age is greater than 30.\n",
    "## 6. Real-World Problem Solving\n",
    "### How can we combat climate change on a global scale?\n",
    "### What are the key factors influencing the real estate market in the Gulf countries?\n",
    "### What is the impact of inflation on the economy?\n",
    "## 7. Creative Tasks\n",
    "### Can you write a short story about a time-traveling scientist?\n",
    "### Write a poem about the beauty of nature.\n",
    "### Suggest a unique idea for a startup business.\n",
    "## 8. Technical Knowledge/Domain-Specific\n",
    "### What are the key features of the Qwen-1.5B model?\n",
    "### Explain the process of quantization in deep learning models.\n",
    "### How does 8-bit quantization affect the performance and accuracy of language models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e922f86f-020c-440c-911c-9f2f89af7b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with the model (type 'exit' to stop):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: hi hi hi, hihihi, hiiii, what's the meaning of this? hi\n",
      "Hi! Hi! Hii! I just wanted to say hi and hi again. Huh, I'm not sure what all this is\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: what is machine learning?\n",
      "</think>\n",
      "\n",
      "**Machine Learning** is a subset of Artificial Intelligence (AI) that focuses on developing algorithms and models that enable computers to perform tasks without explicit programming. Instead of being told what to do, the computer learns from\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# # Function to generate a response based on the recommendations\n",
    "# def chat_with_model(input_text):\n",
    "#     # Encode the input text\n",
    "#     inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "#     # Create an attention mask (ensure it is set for all tokens)\n",
    "#     attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    \n",
    "#     # Apply generation with recommendations for parameters\n",
    "#     outputs = model.generate(inputs,\n",
    "#                              attention_mask=attention_mask,\n",
    "#                              max_length=50,             # Limit the length of the output\n",
    "#                              num_return_sequences=1,    # Generate 1 response\n",
    "#                              pad_token_id=tokenizer.eos_token_id, \n",
    "#                              temperature=0.6,           # Set the temperature to 0.6\n",
    "#                              top_p=0.9,                 # Use nucleus sampling for diversity\n",
    "#                              no_repeat_ngram_size=2)    # Prevent repeating n-grams\n",
    "    \n",
    "#     # Decode the response\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response\n",
    "\n",
    "# # Chat loop to interact with the model\n",
    "# print(\"Chat with the model (type 'exit' to stop):\")\n",
    "# while True:\n",
    "#     user_input = input(\"You: \")\n",
    "#     if user_input.lower() == 'exit':\n",
    "#         print(\"Exiting the chat. Goodbye!\")\n",
    "#         break\n",
    "#     response = chat_with_model(user_input)\n",
    "#     print(f\"Model: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b532d-0c47-4ccd-bfc2-c4623b6b2cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

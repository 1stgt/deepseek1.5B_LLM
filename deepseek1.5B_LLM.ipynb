{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4340b408-b55c-44df-9905-e803444cc269",
   "metadata": {},
   "source": [
    "# Step 1: Install the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff6d620-40f5-4429-9486-d12899069c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 320.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 432.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harshyadav\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/9.7 MB 12.9 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.0/9.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.5/9.7 MB 10.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.9/9.7 MB 10.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.2/9.7 MB 9.9 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.7/9.7 MB 9.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.4/9.7 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.7 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.5/9.7 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.7 MB 11.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.8/9.7 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.4/9.7 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.8/9.7 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.5/9.7 MB 12.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "   ---------------------------------------- 0.0/450.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 450.7/450.7 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "   ---------------------------------------- 0.0/303.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 303.8/303.8 kB 19.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.7/2.4 MB 20.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 13.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.27.1 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch tensorflow flax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db225f-136e-452a-bec5-8fdd4aecaa87",
   "metadata": {},
   "source": [
    "# Step 2: Importing  the library for the \"DeepSeek-R1-Distill-Qwen-1.5B\" model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "440065df-54e5-4946-9457-af3c81054404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee79bf08-83d4-4ca1-9a24-d6d92f1aa738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6f0bfb-bd5e-4cf7-af5b-f58f069795ce",
   "metadata": {},
   "source": [
    "## Download the model and tokenizer\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c2e095-27e3-4ac4-a7d3-a91538f4db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b219a8-3f9c-4edd-a178-8a45b35442db",
   "metadata": {},
   "source": [
    "# saving the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6440c5a4-a31e-47ef-99ec-42641e515fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the local directory where you want to save the model\n",
    "local_model_directory = \".\"  # This will save the model in the current directory\n",
    "\n",
    "# Save the model and tokenizer to the local directory\n",
    "model.save_pretrained(local_model_directory)\n",
    "tokenizer.save_pretrained(local_model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f58bce-4490-4981-829b-87fe9ed56d23",
   "metadata": {},
   "source": [
    "# Loading the Model and Tokenizer from our system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6e57528-e2e5-44ed-a5ea-884dd3fa24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model and tokenizer from the local directory\n",
    "# model = AutoModel.from_pretrained(local_model_directory)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(local_model_directory)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c95427a-b3bd-4158-9ebb-25045061e77b",
   "metadata": {},
   "source": [
    "# Now letâ€™s try to generate some response from the model that we have saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3160f041-0402-4187-9326-37041bcec743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(input_text):\n",
    "    # Encode the input text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    # Create an attention mask (ensure it is set for all tokens)\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    \n",
    "    # Apply generation with more refined parameters\n",
    "    outputs = model.generate(inputs,\n",
    "                             attention_mask=attention_mask,\n",
    "                             max_length=50,            # Allow for more room in the response\n",
    "                             num_return_sequences=1,    # Generate 1 response\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             temperature=0.5,           # Set the temperature lower for more controlled output\n",
    "                             top_p=0.9,                 # Use nucleus sampling for diversity\n",
    "                             top_k=50,                  # Restrict sampling to top 50 most likely next tokens\n",
    "                             no_repeat_ngram_size=3)    # Prevent repeating n-grams\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "def0e9de-cdcd-49ef-805b-874d3662d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baecb26b-0003-4f59-802b-e8549ee74ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with the model (type 'exit' to stop):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: what is tensors in machine learning?\n",
      "\n",
      "What is the difference between tensors and numpy arrays in machine Learning?\n",
      "\n",
      "What are the use cases for tensors in ML?\n",
      "\n",
      "What's the difference in terms of computational efficiency between tensors vs numpy arrays?\n",
      "\n",
      "What about the use\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  explain what is tensors in deep learninig\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: explain what is tensors in deep learninig.\n",
      "\n",
      "Explain what is the difference between tensor and numpy.\n",
      "\n",
      "Examine the following code:\n",
      "\n",
      "import numpy as np\n",
      "a = np.array([[1,2], [3,4]])\n",
      "b = a\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Chat loop\n",
    "print(\"Chat with the model (type 'exit' to stop):\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the chat. Goodbye!\")\n",
    "        break\n",
    "    response = chat_with_model(user_input)\n",
    "    print(f\"Model: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffd155f1-cbab-41a2-9e83-60daeb87afbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42174783-9bf9-4204-b892-68c31d14bf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e922f86f-020c-440c-911c-9f2f89af7b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat with the model (type 'exit' to stop):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: hi hi hi, hihihi, hiiii, what's the meaning of this? hi\n",
      "Hi! Hi! Hii! I just wanted to say hi and hi again. Huh, I'm not sure what all this is\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is machine learning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: what is machine learning?\n",
      "</think>\n",
      "\n",
      "**Machine Learning** is a subset of Artificial Intelligence (AI) that focuses on developing algorithms and models that enable computers to perform tasks without explicit programming. Instead of being told what to do, the computer learns from\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting the chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# # Function to generate a response based on the recommendations\n",
    "# def chat_with_model(input_text):\n",
    "#     # Encode the input text\n",
    "#     inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "#     # Create an attention mask (ensure it is set for all tokens)\n",
    "#     attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    \n",
    "#     # Apply generation with recommendations for parameters\n",
    "#     outputs = model.generate(inputs,\n",
    "#                              attention_mask=attention_mask,\n",
    "#                              max_length=50,             # Limit the length of the output\n",
    "#                              num_return_sequences=1,    # Generate 1 response\n",
    "#                              pad_token_id=tokenizer.eos_token_id, \n",
    "#                              temperature=0.6,           # Set the temperature to 0.6\n",
    "#                              top_p=0.9,                 # Use nucleus sampling for diversity\n",
    "#                              no_repeat_ngram_size=2)    # Prevent repeating n-grams\n",
    "    \n",
    "#     # Decode the response\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return response\n",
    "\n",
    "# # Chat loop to interact with the model\n",
    "# print(\"Chat with the model (type 'exit' to stop):\")\n",
    "# while True:\n",
    "#     user_input = input(\"You: \")\n",
    "#     if user_input.lower() == 'exit':\n",
    "#         print(\"Exiting the chat. Goodbye!\")\n",
    "#         break\n",
    "#     response = chat_with_model(user_input)\n",
    "#     print(f\"Model: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ca6d268-4ef0-449a-8da0-25b69b8fdcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# class ChatBot:\n",
    "#     def __init__(self):\n",
    "#         self.model = None\n",
    "#         self.tokenizer = None\n",
    "\n",
    "#     def load_model(self, model_name: str):\n",
    "#         try:\n",
    "#             self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#             print(f\"Loaded {model_name} model\")\n",
    "#         except FileNotFoundError:\n",
    "#             print(\"No such model found.\")\n",
    "\n",
    "#     def save_model(self, model_name: str):\n",
    "#         torch.save(self.model.state_dict)\n",
    "\n",
    "#     def chat(self, user_input: str):\n",
    "#         if not user_input.strip():\n",
    "#             return f\"No input provided. Please type something.\"\n",
    "\n",
    "#         try:\n",
    "#             self.tokenizer.add_to_buffer(user_input)\n",
    "#             response = self._process_user_input()\n",
    "#             print(f\"Model: {self.model}\")\n",
    "#             print(f\"Response: {response}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "#     def _process_user_input(self):\n",
    "#         user_input = input(\"You: \")\n",
    "#         # Add basic validation to ensure it's not empty\n",
    "#         return user_input\n",
    "\n",
    "# # Usage\n",
    "# chat_bot = ChatBot()\n",
    "# while True:\n",
    "#     chat_bot.chat(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b532d-0c47-4ccd-bfc2-c4623b6b2cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
